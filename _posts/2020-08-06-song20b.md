---
title: Co-training for Policy Learning
abstract: We study the problem of learning sequential decision-making policies in
  settings with multiple state-action representations. Such settings naturally arise
  in many domains, such as planning (e.g., multiple integer programming formulations)
  and various combinatorial optimization problems (e.g., those with both integer programming
  and graph-based formulations). Inspired by the classical co-training framework for
  classification, we study the problem of co-training for policy learning. We present
  sufficient conditions under which learning from two views can improve upon learning
  from a single view alone. Motivated by these theoretical insights, we present a
  meta-algorithm for co-training for sequential decision making. Our framework is
  compatible with both reinforcement learning and imitation learning. We validate
  the effectiveness of our approach across a wide range of tasks, including discrete/continuous
  control and combinatorial optimization.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: song20b
month: 0
tex_title: Co-training for Policy Learning
firstpage: 1191
lastpage: 1201
page: 1191-1201
order: 1191
cycles: false
bibtex_author: Song, Jialin and Lanka, Ravi and Yue, Yisong and Ono, Masahiro
author:
- given: Jialin
  family: Song
- given: Ravi
  family: Lanka
- given: Yisong
  family: Yue
- given: Masahiro
  family: Ono
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/song20b/song20b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v115/song20b/song20b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
