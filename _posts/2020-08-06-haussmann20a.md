---
title: Sampling-Free Variational Inference of Bayesian Neural Networks by Variance
  Backpropagation
abstract: "  We propose a new Bayesian Neural Net formulation that affords variational
  inference for which the evidence lower bound is analytically tractable subject to
  a tight approximation. We achieve this tractability by (i) decomposing ReLU nonlinearities
  into the product of an identity and a Heaviside step function, (ii) introducing
  a separate path that decomposes the neural net expectation from its variance. We
  demonstrate formally that introducing separate latent binary variables to the activations
  allows representing the neural network likelihood as a chain of linear operations.
  Performing variational inference on this construction enables a sampling-free computation
  of the evidence lower bound which is a more effective approximation than the widely
  applied Monte Carlo sampling and CLT related techniques. We evaluate the model on
  a range of regression and classification tasks against BNN inference alternatives,
  showing competitive or improved performance over the current state-of-the-art. "
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: haussmann20a
month: 0
tex_title: Sampling-Free Variational Inference of Bayesian Neural Networks by Variance
  Backpropagation
firstpage: 563
lastpage: 573
page: 563-573
order: 563
cycles: false
bibtex_author: Hau{\ss}mann, Manuel and Hamprecht, Fred A. and Kandemir, Melih
author:
- given: Manuel
  family: Hau√ümann
- given: Fred A.
  family: Hamprecht
- given: Melih
  family: Kandemir
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/haussmann20a/haussmann20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
