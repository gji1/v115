---
title: Deep Mixture of Experts via Shallow Embedding
abstract: Larger networks generally have greater representational power at the cost
  of increased computational complexity. Sparsifying such networks has been an active
  area of research but has been generally limited to static regularization or dynamic
  approaches using reinforcement learning. We explore a mixture of experts (MoE) approach
  to deep dynamic routing, which activates certain experts in the network on a per-example
  basis. Our novel DeepMoE architecture increases the representational power of standard
  convolutional networks by adaptively sparsifying and recalibrating channel-wise
  features in each convolutional layer.  We employ a multi-headed sparse gating network
  to determine the selection and scaling of channels for each input, leveraging exponential
  combinations of experts within a single convolutional network. Our proposed architecture
  is evaluated on four benchmark datasets and tasks, and we show that Deep-MoEs are
  able to achieve higher accuracy with lower computation than standard convolutional
  networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: wang20d
month: 0
tex_title: Deep Mixture of Experts via Shallow Embedding
firstpage: 552
lastpage: 562
page: 552-562
order: 552
cycles: false
bibtex_author: Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth
  and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E.
author:
- given: Xin
  family: Wang
- given: Fisher
  family: Yu
- given: Lisa
  family: Dunlap
- given: Yi-An
  family: Ma
- given: Ruth
  family: Wang
- given: Azalia
  family: Mirhoseini
- given: Trevor
  family: Darrell
- given: Joseph E.
  family: Gonzalez
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/wang20d/wang20d.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v115/wang20d/wang20d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
