---
title: Comparing EM with GD in Mixture Models of Two Components
abstract: 'The expectation-maximization (EM) algorithm has been widely used in minimizing
  the negative log likelihood (also known as cross entropy) of mixture models. However,
  little is understood about the goodness of the fixed points it converges to. In
  this paper, we study the regions where one component is missing in two-component
  mixture models, which we call one-cluster regions. We analyze the propensity of
  such regions to trap EM and gradient descent (GD) for mixtures of two Gaussians
  and mixtures of two Bernoullis. In the case of Gaussian mixtures, EM escapes one-cluster
  regions exponentially fast, while GD escapes them linearly fast. In the case of
  mixtures of Bernoullis, we find that there exist one-cluster regions that are stable
  for GD and therefore trap GD, but those regions are unstable for EM, allowing {EM}  to
  escape. Those regions are local minima that appear universally in experiments and
  can be arbitrarily bad. This work implies that EM is less likely than GD to converge
  to certain bad local optima in mixture models. '
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: zhang20a
month: 0
tex_title: Comparing EM with GD in Mixture Models of Two Components
firstpage: 164
lastpage: 174
page: 164-174
order: 164
cycles: false
bibtex_author: Zhang, Guojun and Poupart, Pascal and Trimponias, George
author:
- given: Guojun
  family: Zhang
- given: Pascal
  family: Poupart
- given: George
  family: Trimponias
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/zhang20a/zhang20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v115/zhang20a/zhang20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
