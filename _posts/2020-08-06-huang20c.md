---
title: 'Probability Distillation: A Caveat and Alternatives'
abstract: 'Due to Van den Oord et al. (2018), probability distillation has recently
  been of interest to deep learning practitioners, where,  as a practical workaround
  for deploying autoregressive models in real-time applications, a student net-work
  is used to obtain quality samples in parallel.  We identify a pathological optimization
  issue with the adopted stochastic minimization of the reverse-KL divergence:  the
  curse of dimensionality results in a skewed gradient distribution that renders training
  inefficient.  This means that KL-based "evaluative" training can be susceptible
  to poor exploration if the target distribution is highly structured.  We then explore
  alternative principles for distillation, including one with an "instructive" signal,  and
  show that it is possible to achieve qualitatively better results than with KL minimization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: huang20c
month: 0
tex_title: 'Probability Distillation: A Caveat and Alternatives'
firstpage: 1212
lastpage: 1221
page: 1212-1221
order: 1212
cycles: false
bibtex_author: Huang, Chin-Wei and Ahmed, Faruk and Kumar, Kundan and Lacoste, Alexandre
  and Courville, Aaron
author:
- given: Chin-Wei
  family: Huang
- given: Faruk
  family: Ahmed
- given: Kundan
  family: Kumar
- given: Alexandre
  family: Lacoste
- given: Aaron
  family: Courville
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/huang20c/huang20c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
