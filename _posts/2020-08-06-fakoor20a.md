---
title: 'P3O: Policy-on Policy-off Policy Optimization'
abstract: On-policy reinforcement learning (RL) algorithms have high sample complexity
  while off-policy algorithms are difficult to tune. Merging the two holds the promise
  to develop efficient algorithms that generalize across diverse environments. It
  is however challenging in practice to find suitable hyper-parameters that govern
  this trade off. This paper develops a simple algorithm named P3O that interleaves
  off-policy updates with on-policy updates. P3O uses the effective sample size between
  the behavior policy and the target policy to control how far they can be from each
  other and does not introduce any additional hyper-parameters. Extensive experiments
  on the Atari-2600 and MuJoCo benchmark suites show that this simple technique is
  effective in reducing the sample complexity of state-of-the-art algorithms. Code
  to reproduce experiments in this paper is at https://github.com/rasoolfa/P3O.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: fakoor20a
month: 0
tex_title: 'P3O: Policy-on Policy-off Policy Optimization'
firstpage: 1017
lastpage: 1027
page: 1017-1027
order: 1017
cycles: false
bibtex_author: Fakoor, Rasool and Chaudhari, Pratik and Smola, Alexander J.
author:
- given: Rasool
  family: Fakoor
- given: Pratik
  family: Chaudhari
- given: Alexander J.
  family: Smola
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/fakoor20a/fakoor20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v115/fakoor20a/fakoor20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
