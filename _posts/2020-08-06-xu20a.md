---
title: An Improved Convergence Analysis of Stochastic Variance-Reduced Policy Gradient
abstract: 'We revisit the stochastic variance-reduced policy gradient (SVRPG) method
  proposed by \citet{papini2018stochastic} for reinforcement learning. We provide
  an improved convergence analysis of SVRPG and show that it can find an $\epsilon$-approximate
  stationary point of the performance function within $O(1/\epsilon^{5/3})$ trajectories.
  This sample complexity improves upon the best known result $O(1/\epsilon^2)$ by
  a factor of $O(1/\epsilon^{1/3})$. At the core of our analysis is (i) a tighter
  upper bound for the variance of importance sampling weights, where we prove that
  the variance can be controlled by the parameter distance between different policies;
  and (ii) a fine-grained analysis of the epoch length and batch size parameters such
  that we can significantly reduce the number of trajectories required in each iteration
  of SVRPG. We also empirically demonstrate the effectiveness of our theoretical claims
  of batch sizes on  reinforcement learning benchmark tasks. '
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: xu20a
month: 0
tex_title: An Improved Convergence Analysis of Stochastic Variance-Reduced Policy
  Gradient
firstpage: 541
lastpage: 551
page: 541-551
order: 541
cycles: false
bibtex_author: Xu, Pan and Gao, Felicia and Gu, Quanquan
author:
- given: Pan
  family: Xu
- given: Felicia
  family: Gao
- given: Quanquan
  family: Gu
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/xu20a/xu20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
