---
title: Perturbed-History Exploration in Stochastic Linear Bandits
abstract: We propose a new online algorithm for cumulative regret minimization in
  a stochastic linear bandit. The algorithm pulls the arm with the highest estimated
  reward in a linear model trained on its perturbed history. Therefore, we call it
  perturbed-history exploration in a linear bandit (LinPHE). The perturbed history
  is a mixture of observed rewards and randomly generated i.i.d. pseudo-rewards. We
  derive a $\tilde{O}(d \sqrt{n})$ gap-free bound on the $n$-round regret of LinPHE,
  where $d$ is the number of features. The key steps in our analysis are new concentration
  and anti-concentration bounds on the weighted sum of Bernoulli random variables.
  To show the generality of our design, we generalize LinPHE to a logistic model.
  We evaluate our algorithms empirically and show that they are practical.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: kveton20a
month: 0
tex_title: Perturbed-History Exploration in Stochastic Linear Bandits
firstpage: 530
lastpage: 540
page: 530-540
order: 530
cycles: false
bibtex_author: Kveton, Branislav and Szepesv{\'{a}}ri, Csaba and Ghavamzadeh, Mohammad
  and Boutilier, Craig
author:
- given: Branislav
  family: Kveton
- given: Csaba
  family: Szepesv√°ri
- given: Mohammad
  family: Ghavamzadeh
- given: Craig
  family: Boutilier
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/kveton20a/kveton20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
