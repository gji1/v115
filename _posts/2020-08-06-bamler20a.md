---
title: Augmenting and Tuning Knowledge Graph Embeddings
abstract: Knowledge graph embeddings rank among the most successful methods for link
  prediction in knowledge graphs, i.e., the task of completing an incomplete collection
  of relational facts. A downside of these models is their strong sensitivity to model
  hyperparameters, in particular regularizers, which have to be extensively tuned
  to reach good performance [Kadlec et al., 2017]. We propose an efficient method
  for large scale hyperparameter tuning by interpreting these models in a probabilistic
  framework. After a model augmentation that introduces per-entity hyperparameters,
  we use a variational expectation-maximization approach to tune thousands of such
  hyperparameters with minimal additional cost. Our approach is agnostic to details
  of the model and results in a new state of the art in link prediction on standard
  benchmark data.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: bamler20a
month: 0
tex_title: Augmenting and Tuning Knowledge Graph Embeddings
firstpage: 508
lastpage: 518
page: 508-518
order: 508
cycles: false
bibtex_author: Bamler, Robert and Salehi, Farnood and Mandt, Stephan
author:
- given: Robert
  family: Bamler
- given: Farnood
  family: Salehi
- given: Stephan
  family: Mandt
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/bamler20a/bamler20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v115/bamler20a/bamler20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
