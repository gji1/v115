---
title: Random Search and Reproducibility for Neural Architecture Search
abstract: 'Neural architecture search (NAS) is a promising research direction that
  has the potential to replace expert-designed networks with learned, task-specific
  architectures. In order to help ground the empirical results in this field, we propose
  new NAS baselines that build off the following observations: (i) NAS is a specialized
  hyperparameter optimization problem; and (ii) random search is a competitive baseline
  for hyperparameter optimization. Leveraging these observations, we evaluate both
  random search with early-stopping and a novel random search with weight-sharing
  algorithm on two standard NAS benchmarksâ€”PTB and CIFAR-10. Our results show that
  random search with early-stopping is a competitive NAS baseline, e.g., it performs
  at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally,
  random search with weight-sharing outperforms random search with early-stopping,
  achieving a state-of-the-art NAS result on PTB and a highly competitive result on
  CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS
  results.'
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: li20c
month: 0
tex_title: Random Search and Reproducibility for Neural Architecture Search
firstpage: 367
lastpage: 377
page: 367-377
order: 367
cycles: false
bibtex_author: Li, Liam and Talwalkar, Ameet
author:
- given: Liam
  family: Li
- given: Ameet
  family: Talwalkar
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/li20c/li20c.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v115/li20c/li20c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
