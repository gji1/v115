---
title: Off-Policy Policy Gradient with Stationary Distribution Correction
abstract: We study the problem of off-policy policy optimization in Markov decision
  processes, and develop a novel off-policy policy gradient method. Prior off-policy
  policy gradient approaches have generally ignored the mismatch between the distribution
  of states visited under the behavior policy used to collect data, and what would
  be the distribution of states under the learned policy. Here we build on recent
  progress for estimating the ratio of the state distributions under behavior and
  evaluation policies for policy evaluation, and present an off-policy policy gradient
  optimization technique that can account for this mismatch in distributions. We present
  an illustrative example of why this is important and a theoretical convergence guarantee
  for our approach. Empirically, we compare our method in simulations to several strong
  baselines which do not correct for this mismatch, significantly improving in the
  quality of the policy discovered.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: liu20a
month: 0
tex_title: Off-Policy Policy Gradient with Stationary Distribution Correction
firstpage: 1180
lastpage: 1190
page: 1180-1190
order: 1180
cycles: false
bibtex_author: Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma
author:
- given: Yao
  family: Liu
- given: Adith
  family: Swaminathan
- given: Alekh
  family: Agarwal
- given: Emma
  family: Brunskill
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/liu20a/liu20a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v115/liu20a/liu20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
