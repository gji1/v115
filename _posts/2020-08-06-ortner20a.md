---
title: Variational Regret Bounds for Reinforcement Learning
abstract: 'We consider undiscounted reinforcement learning in Markov decision processes
  (MDPs) where \textit{both} the reward functions and the state-transition probabilities
  may vary (gradually or abruptly) over time.  For this problem setting, we propose
  an algorithm and provide performance guarantees for the regret evaluated against
  the optimal non-stationary policy. The upper bound on the regret is given in terms
  of the total variation in the MDP. This is the first variational regret bound for
  the general reinforcement learning setting. '
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: ortner20a
month: 0
tex_title: Variational Regret Bounds for Reinforcement Learning
firstpage: 81
lastpage: 90
page: 81-90
order: 81
cycles: false
bibtex_author: Ortner, Ronald and Gajane, Pratik and Auer, Peter
author:
- given: Ronald
  family: Ortner
- given: Pratik
  family: Gajane
- given: Peter
  family: Auer
date: 2020-08-06
address: 
publisher: PMLR
container-title: Proceedings of The 35th Uncertainty in Artificial Intelligence Conference
volume: '115'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 6
pdf: http://proceedings.mlr.press/v115/ortner20a/ortner20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
